
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How to Translate a Math PDF (Co-written with AI)</title>
    <style>
        /* Basic styling for readability */
        body { font-family: sans-serif; line-height: 1.6; margin: 2em; max-width: 900px; margin-left: auto; margin-right: auto; }
        h1, h2, h3, h4 { margin-top: 1.5em; color: #333; }
        h1 { border-bottom: 2px solid #eee; padding-bottom: 0.3em;}
        pre { background-color: #f8f8f8; padding: 1em; border: 1px solid #ddd; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word; font-size: 0.9em; border-radius: 4px; }
        code { font-family: monospace; background-color: #f0f0f0; padding: 0.2em 0.4em; border-radius: 3px;}
        ul, ol { margin-left: 1.5em; }
        li { margin-bottom: 0.5em; }
        a { color: #007bff; text-decoration: none; }
        a:hover { text-decoration: underline; }
        /* Style to visually separate AI's explanation */
        .ai-explanation { border-left: 4px solid #4285F4; /* Google blue */ padding-left: 1em; margin-top: 1.5em; margin-bottom: 1.5em; background-color: #f9f9f9; border-radius: 4px; padding: 1em; }
        .requirements ul li strong { display: inline-block; min-width: 150px; } /* Align requirement titles */
        .warning { color: #dc3545; font-weight: bold; }
        .note { font-style: italic; color: #555; }
    </style>
</head>
<body>

<h1>How to Translate a Math PDF (Co-written with Gemini)</h1>

<p>Modern multimodal AI models can take a math paper in a non-English language (provided as a PDF) and output a translation directly in LaTeX. This guide explains how to use a Python script that leverages multiple AI providers (Anthropic Claude, OpenAI GPT, Google Gemini) sequentially to achieve this.</p>

<p><span class="warning">Disclaimer:</span> AI translations, especially for complex technical documents, carry risks of hallucination or error. This guide and the accompanying script were developed collaboratively with AI and reflect capabilities as of early 2025; they may require updates as APIs and models evolve. Always carefully review the generated LaTeX output.</p>

<p>The core translation ability resides within the AI models themselves. This script provides scaffolding to overcome common challenges:</p>
<ul>
    <li><strong>Output Length Limits:</strong> AIs often struggle to generate LaTeX for very long papers in a single API call.</li>
    <li><strong>API Reliability/Failures:</strong> Individual API calls can fail. Using multiple models increases robustness.</li>
    <li><strong>Consistency:</strong> Maintaining consistent LaTeX formatting and translation style across many pages can be difficult for AIs.</li>
    <li><strong>Compilation Errors:</strong> AI-generated LaTeX might contain errors that prevent compilation.</li>
</ul>

<p>We divide this guide into two main situations.</p>

<h2>Situation 1 (Easiest): Translating Short Papers (e.g., < 20 pages)</h2>

<p>For short papers, direct translation via a chat interface of a capable multimodal AI (like Google AI Studio, Anthropic's Console, or OpenAI's ChatGPT with vision) is often sufficient.</p>
<ol>
    <li><strong>Method:</strong> Upload the PDF directly into the chat interface.</li>
    <li><strong>Prompt:</strong> Use a detailed prompt like:
        <pre><code>Please translate the uploaded non-English math document into English LaTeX code. Preserve all mathematical formulas, equations, and environments accurately using standard LaTeX (amsmath, amssymb, etc.). Generate a complete LaTeX document, including a suitable preamble (documentclass, common packages like graphicx, amsthm, hyperref), metadata (title, author if visible), and the translated content.</code></pre>
    </li>
    <li><strong>Process:</strong> Copy the AI's generated LaTeX, paste it into a LaTeX editor (e.g., Overleaf, VS Code with LaTeX Workshop), compile, and meticulously review the resulting PDF for correctness and formatting.</li>
</ol>

<h2>Situation 2: Translation for Longer Papers via the Multi-API Script</h2>

<p>For longer papers, the provided Python script (`translator.py`) automates a page-by-page translation process using a sequence of different AI models.</p>

<p>The script workflow:</p>
<ol>
    <li>Reads processing state (if resuming).</li>
    <li>Iterates through PDF pages, converting each to an image.</li>
    <li>For each page image, it attempts translation using multiple models:
        <ul>
            <li>Sends the image and contextual information (like previous page content) to models from different providers (Claude, GPT, Gemini) based on a defined sequence (queue order).</li>
            <li>Uses the LaTeX output from the first model in the sequence that successfully responds.</li>
            <li>Includes retries and cycles through the model sequence if initial attempts fail.</li>
        </ul>
    </li>
    <li>If all models fail for a page, it inserts a placeholder into the LaTeX document and saves the original page image for manual review.</li>
    <li>Appends the successful LaTeX (or placeholder) for the current page to the cumulative document.</li>
    <li>Attempts to compile the cumulative LaTeX document using `pdflatex` after each page.</li>
    <li>If compilation succeeds, saves the state and the `.tex`/`.pdf` files.</li>
    <li>If compilation fails (even with a placeholder), it indicates a critical error and stops processing.</li>
    <li>At the end, generates a report summarizing model performance and optionally merges it with the main PDF.</li>
</ol>

<p>We provide the <a href="translator.py" download>example Python script (translator.py)</a>.
<!-- Make sure translator.py is in the same directory -->
To use this script, you need API keys for the AI providers you wish to use and a correctly configured Python environment.</p>

<p>Here's a more detailed explanation:</p>

<div class="ai-explanation"> <!-- Start of AI Explanation -->
    <h3>How to Use the Multi-Model Python Script (translator.py)</h3>

    <h4>1. Requirements:</h4>
    <ul class="requirements">
        <li><strong>Python:</strong> Version 3.x recommended. Use a virtual environment (`venv`).</li>
        <li><strong>Python Packages:</strong> Install via pip:
            <pre><code>pip install google-generativeai anthropic openai pdf2image Pillow pypdf</code></pre>
        </li>
        <li><strong>Poppler:</strong> Required by `pdf2image`. Install system-wide (e.g., `brew install poppler`, `sudo apt-get install poppler-utils`). Ensure it's in your PATH. <span class="warning">Installation issues are common!</span></li>
        <li><strong>LaTeX Distribution:</strong> A working installation (TeX Live, MiKTeX) is **required**. The `pdflatex` command must be runnable from your terminal.</li>
        <li><strong>API Keys:</strong> You need API keys for the providers you intend to use. Set them as environment variables:
            <ul>
                <li><code>ANTHROPIC_API_KEY</code> (for Claude models)</li>
                <li><code>OPENAI_API_KEY</code> (for GPT models)</li>
                <li><code>GOOGLE_API_KEY</code> (for Gemini models)</li>
            </ul>
             Get keys from their respective websites (Anthropic Console, OpenAI Platform, Google AI Studio). The script will skip models from providers whose keys are not found.
        </li>
    </ul>

    <h4>2. Configuration (Optional):</h4>
    <ul>
        <li><strong>Model Sequence/Configs:</strong> Edit the `get_model_configs()` function and `DEFAULT_MODEL_QUEUE` list within `translator.py` to add/remove models or change their priority in the processing sequence.</li>
        <li><strong>Working Directory:</strong> By default, outputs go to `latex_processing_multi`. Use the `--working-dir` flag to change this.</li>
    </ul>


    <h4>3. Running the Script:</h4>
    <p>Open your terminal/command prompt in the directory containing `translator.py`.</p>
    <p><strong>Basic Command:</strong></p>
    <pre><code>python translator.py [path/to/your/input.pdf] [options...]</code></pre>
    <ul>
        <li>The <strong>Input PDF Path</strong> is the first (optional) argument. If omitted, it defaults to `input.pdf` in the script's directory.</li>
        <li>All other options use flags (e.g., <code>--model</code>, <code>--working-dir</code>).</li>
    </ul>

    <p><strong>Example Commands:</strong></p>
    <pre><code class="language-bash"># Translate my_paper.pdf using default settings
python translator.py my_paper.pdf

# Translate input.pdf, preferring claude-3.7-sonnet first
python translator.py --model claude-3.7-sonnet

# Translate paper_v2.pdf and store outputs in a custom directory
python translator.py paper_v2.pdf --working-dir /path/to/my/latex_output</code></pre>

    <p class="note"><em>Use quotes around paths with spaces. You might need <code>python3</code> instead of <code>python</code>.</em></p>

    <h4>4. Command-Line Options (Flags):</h4>
    <ul>
        <li><code>input_pdf</code> (Positional, Optional): Path to the input PDF (defaults to `input.pdf`).</li>
        <li><code>--model MODEL_KEY</code> or <code>-m MODEL_KEY</code>: Specify a preferred model key (e.g., `claude-3-opus`, `gpt-4o`) to try first in the processing sequence. Use <code>--list-models</code> to see available keys.</li>
        <li><code>--working-dir PATH</code> or <code>-w PATH</code>: Set the directory for all outputs (logs, state, .tex, .pdf, failed pages). Defaults to `./latex_processing_multi`.</li>
        <li><code>--list-models</code> or <code>-l</code>: Display available model keys configured in the script and the default processing sequence order, then exit.</li>
    </ul>

    <h4>5. Output Files (in Working Directory):</h4>
    <ul>
        <li><code>cumulative_good.tex</code>: The latest successfully compiled LaTeX source.</li>
        <li><code>cumulative_good.pdf</code>: The PDF generated from `cumulative_good.tex`.</li>
        <li><code>processing.log</code>: Detailed log of the translation process.</li>
        <li><code>state.json</code>: Stores the last completed page for resuming.</li>
        <li><code>model_stats.json</code>: Tracks which models successfully translated which pages and records failures.</li>
        <li><code>latex_aux/</code>: Directory containing auxiliary files from LaTeX compilation (`.log`, `.aux`, etc.).</li>
        <li><code>failed_pages/</code>: Contains images (`.jpg`) of pages that failed all translation attempts.</li>
        <li><code>*.log</code> (e.g., `compile_fail_*.log`, `report_compile.log`): Specific logs for compilation failures.</li>
        <li><code>*final.pdf</code> / <code>*final_with_report.pdf</code>: The final output PDF, potentially merged with reports/appendices.</li>
    </ul>

    <h4>6. Resuming and Error Handling:</h4>
    <ul>
        <li><strong>Resuming:</strong> Simply run the script again with the same input PDF and working directory. It will read `state.json` and continue from the last successfully completed page.</li>
        <li><strong>Failures:</strong>
            <ul>
                <li>If a page fails all model translation attempts, its image is saved in `failed_pages/`, a placeholder is inserted in the LaTeX, and processing continues.</li>
                <li>If the generated LaTeX (even with placeholders) fails to compile, it usually indicates a deeper issue in the document structure. Processing stops, and logs (`compile_fail*.log`) should be checked.</li>
                <li>Fix errors in `cumulative_good.tex` manually, then rerun the script to resume from the corrected state.</li>
            </ul>
        </li>
    </ul>


    <h3>Key Features of the Multi-Model Script</h3>
    <ul>
        <li><strong>Multi-Provider Processing:</strong> Leverages Claude, GPT, and Gemini models sequentially for robustness and potentially better results.</li>
        <li><strong>Configurable Model Sequence:</strong> Allows prioritizing preferred models in the processing order.</li>
        <li><strong>Page-by-Page Processing:</strong> Handles long documents by processing one page at a time.</li>
        <li><strong>Image-Based Translation:</strong> Converts PDF pages to images for multimodal AI input.</li>
        <li><strong>Contextual Prompting:</strong> Provides models with context from the previous page's LaTeX output to improve consistency.</li>
        <li><strong>Compilation Checks:</strong> Attempts `pdflatex` compilation after each page to catch errors early.</li>
        <li><strong>Graceful Failure Handling:</strong> Inserts placeholders for pages that fail translation or cause compilation errors, allowing processing to continue where possible. Saves images of failed pages.</li>
        <li><strong>State Management:</strong> Automatically saves and resumes progress using `state.json`.</li>
        <li><strong>Performance Reporting:</strong> Generates statistics (`model_stats.json`) and a final PDF report summarizing which models were used and which pages failed.</li>
        <li><strong>Command-Line Interface:</strong> Flexible control via CLI arguments.</li>
        <li><strong>Detailed Logging:</strong> Records progress and errors to `processing.log`.</li>
    </ul>
</div> <!-- End of ai-explanation div -->

</body>
</html>
```
