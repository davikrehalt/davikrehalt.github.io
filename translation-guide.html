<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How to Translate a Math PDF (Co-written with Gemini 2.5 Pro)</title>
    <style>
        /* Basic styling for readability */
        body { font-family: sans-serif; line-height: 1.6; margin: 2em; max-width: 900px; margin-left: auto; margin-right: auto; }
        h1, h2, h3, h4 { margin-top: 1.5em; color: #333; }
        h1 { border-bottom: 2px solid #eee; padding-bottom: 0.3em;}
        pre { background-color: #f8f8f8; padding: 1em; border: 1px solid #ddd; overflow-x: auto; white-space: pre-wrap; /* Wrap long lines */ word-wrap: break-word; /* Break words if needed */ font-size: 0.9em; border-radius: 4px; }
        code { font-family: monospace; background-color: #f0f0f0; padding: 0.2em 0.4em; border-radius: 3px;}
        ul, ol { margin-left: 1.5em; }
        li { margin-bottom: 0.5em; }
        a { color: #007bff; text-decoration: none; }
        a:hover { text-decoration: underline; }
        /* Style to visually separate Gemini's explanation */
        .gemini-explanation { border-left: 4px solid #4285F4; /* Google blue */ padding-left: 1em; margin-top: 1.5em; margin-bottom: 1.5em; background-color: #f9f9f9; border-radius: 4px; padding: 1em; }
        .requirements ul li strong { display: inline-block; min-width: 150px; } /* Align requirement titles */
        .warning { color: #dc3545; font-weight: bold; }
        .note { font-style: italic; color: #555; }
        /* Simple table styling for comparison (No longer used) */
        /* table.comparison { border-collapse: collapse; width: 100%; margin-top: 1.5em; }
        table.comparison th, table.comparison td { border: 1px solid #ddd; padding: 8px; text-align: left; vertical-align: top;}
        table.comparison th { background-color: #f2f2f2; } */
    </style>
</head>
<body>

<h1>How to Translate a Math PDF (Co-written with Gemini 2.5 Pro)</h1>

<p>Nowadays, multimodal AI models can take as input a math paper in a non-English language and output a translation directly in LaTeX. Below is a guide on this. Before proceeding, beware that there are always risks of hallucination or error from the AI and from the fact that this guide was written in April 2025 and was suboptimal even when written.</p> 

<p>A quick note: for this to work, the core capability for this task has to exist in the multimodal AI itself. The guide below merely provides scaffolding to overcome the following common challenges:</p>
<ul>
    <li><strong>Output Length Limits:</strong> AIs often can't generate the entire LaTeX for a long paper in one output, especially via API calls.</li>
    <li><strong>AI Confusion from Input Size / Context Limits:</strong> Processing hundreds of pages of math can sometimes lead to a degradation in translation ability or increased errors.</li>
</ul>

<p>Gemini notes that beyond the challenges mentioned above, achieving consistent formatting, handling complex layouts (multi-column, figures), and ensuring perfect mathematical fidelity remain difficult.</p>

<p>We divide this guide into two main situations.</p>

<h2>Situation 1 (Easiest): Translating Short Papers (e.g., < 20 pages)</h2>

<p>If the paper is short, you can often translate it directly using a chat interface for any capable multimodal AI model (like Google's AI Studio for Gemini, Anthropic's Console for Claude, or OpenAI's ChatGPT).</p>
<ol>
    <li><strong>Method:</strong> Upload the PDF directly into the chat interface.</li>
    <li><strong>Prompt:</strong> Use a prompt like this:
        <pre><code>Please translate the uploaded document into English LaTeX code. Ensure all mathematical formulas and environments are preserved correctly. Generate the complete LaTeX document, including a suitable preamble.</code></pre>
    </li>
    <li><strong>Process:</strong> Copy the AI's generated LaTeX output, paste it into your LaTeX editor (e.g., Overleaf), compile, and carefully review the resulting PDF.</li>
</ol>

<h2>Situation 2: Translation for Longer Papers via an API Script</h2>

<p>For longer papers, we must divide the paper into manageable chunks (groups of pages) for translation. Using AI APIs (Anthropic, Google, OpenAI), it is possible to write code to automate this process. The philosophy here is that the scaffolding imposed on the AI model should be minimal, yet robust enough to handle failures and manage context effectively by processing the document in semantically relevant chunks.</p>

<p>We provide an example script co-written with Gemini:</p>
<ul>
    <li><a href="translator.py" download>`translator.py`</a>: Uses a **Chunk-Based, AI-Assisted Synthesis** approach.</li>
</ul>
<!-- Make sure translator.py is in the same directory as this HTML file, or adjust the href path -->
<p>In order to use this script, you first need to obtain an API key for the relevant AI provider and set up a Python environment with the necessary libraries installed.</p>

<p>I'll let Gemini explain the usage of this script in more detail:</p>

<!-- ================================================================= -->
<!-- Explanation for Script: translator.py (Chunk-Based Synthesis)         -->
<!-- ================================================================= -->
<div class="gemini-explanation">
    <h3>Script: Chunk-Based AI Translation (`translator.py`)</h3>
    <p>This script divides the input PDF into logical chunks (using AI analysis or fixed page counts), translates each chunk individually using a *single selected AI model*, and then attempts to synthesize a coherent final LaTeX document. It extracts the preamble and body from each successfully compiled chunk and uses AI again to merge the preambles intelligently.</p>

    <h4>1. Requirements:</h4>
    <ul class="requirements">
        <li><strong>Python:</strong> Version 3.x recommended. Use a virtual environment (`venv`).</li>
        <li><strong>Python Packages:</strong> Install via pip:
            <pre><code>pip install google-generativeai openai anthropic pypdf python-dotenv httpx</code></pre>
            <p class="note">(Note: `pdf2image` and `Pillow` are no longer needed as the script uploads PDF chunks directly if the model supports it).</p>
        </li>
        <!-- Poppler is no longer needed -->
        <li><strong>LaTeX Distribution:</strong> A working installation (TeX Live, MiKTeX) is **required**. The `pdflatex` command must be runnable from your terminal for compilation checks and final output generation.</li>
        <li><strong>API Key:</strong> Set the environment variable for the *provider of the model you intend to use*:
            <ul>
                <li><code>GOOGLE_API_KEY</code> (or <code>GEMINI_API_KEY</code>) for Gemini models</li>
                <li><code>OPENAI_API_KEY</code> for OpenAI models</li>
                <li><code>ANTHROPIC_API_KEY</code> for Claude models</li>
            </ul>
             Get keys from their respective websites (Google AI Studio, OpenAI Platform, Anthropic Console). The script will fail if the key for the selected provider/model is not found.
        </li>
    </ul>

    <h4>2. Configuration (Optional):</h4>
    <ul>
        <li><strong>Model Configs:</strong> Edit `get_model_configs()` in `translator.py` to add/update model details or change defaults in `DEFAULT_MODEL_KEY_MAP`.</li>
        <li><strong>Chunk Size:</strong> Adjust `TARGET_PAGES_PER_CHUNK` constant or use the `--chunk-size` flag (affects fallback chunking if AI definition fails).</li>
        <li><strong>Working Directory:</strong> Defaults to `./latex_processing_llm`. Use the `--working-dir` flag to change this.</li>
        <li><strong>Retries:</strong> Adjust `MAX_CHUNK_RETRIES` for API/compile attempts per chunk.</li>
    </ul>

    <h4>3. Running the Script:</h4>
    <p>Open your terminal/command prompt in the directory containing `translator.py`.</p>
    <p><strong>Basic Command (using provider defaults):</strong></p>
    <pre><code>python translator.py [path/to/your/input.pdf] [--provider PROVIDER] [options...]</code></pre>
    <p><strong>Command with Specific Model:</strong></p>
    <pre><code>python translator.py [path/to/your/input.pdf] --model MODEL_KEY [options...]</code></pre>
    <ul>
        <li>The <strong>Input PDF Path</strong> is the first (optional) argument. If omitted, it defaults to `input.pdf`.</li>
        <li>Use <code>--provider</code> (gemini, openai, claude) to select the AI service.</li>
        <li>Use <code>--model</code> to specify an exact model key (e.g., `gemini-2.5-pro-preview`, `gpt-4o`, `claude-3.7-sonnet`). This overrides the provider's default.</li>
    </ul>

    <p><strong>Example Commands:</strong></p>
    <pre><code class="language-bash"># Translate my_paper.pdf using the default Gemini model
python translator.py my_paper.pdf --provider gemini

# Translate paper_v2.pdf using Claude 3.7 Sonnet specifically
python translator.py paper_v2.pdf --model claude-3.7-sonnet

# Translate input.pdf using OpenAI's gpt-4o, custom chunk size, different working dir
python translator.py --provider openai --model gpt-4o --chunk-size 15 --working-dir /data/gpt4o_translation

# Clean the working directory before starting a fresh run
python translator.py my_paper.pdf --provider gemini --clean</code></pre>

    <p class="note"><em>Use quotes around paths with spaces. You might need <code>python3</code> instead of <code>python</code>. The <code>--clean</code> flag will ask for confirmation before deleting the working directory.</em></p>

    <h4>4. Command-Line Options (Flags):</h4>
    <ul>
        <li><code>input_pdf</code> (Positional, Optional): Path to input PDF (defaults to `input.pdf`).</li>
        <li><code>--provider PROVIDER</code> or <code>-p PROVIDER</code>: AI provider ('gemini', 'openai', 'claude'). Uses the default provider if omitted.</li>
        <li><code>--model MODEL_KEY</code> or <code>-m MODEL_KEY</code>: Specific model key (must belong to the selected provider). Overrides provider default.</li>
        <li><code>--list-models</code> or <code>-l</code>: Display available model keys configured in the script, then exit.</li>
        <li><code>--working-dir PATH</code> or <code>-w PATH</code>: Set the directory for all outputs (logs, state, .tex, .pdf, chunks). Defaults to `./latex_processing_llm`.</li>
        <li><code>--chunk-size N</code> or <code>-c N</code>: Target pages per chunk (for fallback or potentially AI guidance). Defaults to 10.</li>
        <li><code>--clean</code>: Delete the working directory before starting (prompts for confirmation).</li>
    </ul>

     <h4>5. Output Files (in Working Directory):</h4>
    <ul>
        <li><code>*_translated.tex</code>: The final synthesized LaTeX source file (e.g., `my_paper_translated.tex`).</li>
        <li><code>*_translated.pdf</code>: The final compiled PDF output.</li>
        <li><code>processing.log</code>: Detailed log of the entire process.</li>
        <li><code>prompts_and_responses.log</code>: Log of prompts sent to the AI and summaries of responses received.</li>
        <li><code>state.json</code>: Stores the last completed chunk index and other state for resuming.</li>
        <li><code>chunk-division.txt</code>: JSON file containing the determined chunk definitions (page ranges, descriptions).</li>
        <li><code>latex_aux/</code>: Directory containing auxiliary files from all LaTeX compilations (`.log`, `.aux`, etc.).</li>
        <li><code>saved_pdf_chunks/</code>: Contains the individual PDF files created for each chunk sent to the AI.</li>
        <li><code>failed_chunks/</code>: Contains the generated <code>.tex</code> and compilation <code>.log</code> files for chunks that failed all processing attempts.</li>
    </ul>

    <h4>6. Resuming and Error Handling:</h4>
    <ul>
        <li><strong>Resuming:</strong> Run the script again with the same input PDF, provider/model, and working directory. It reads `state.json` and `chunk-division.txt` to continue from the last successfully completed chunk.</li>
        <li><strong>Failures:</strong>
            <ul>
                <li>If an API call fails, the script retries up to `MAX_CHUNK_RETRIES` times with backoff.</li>
                <li>If the LaTeX generated for a chunk fails to compile, the script also retries compilation (potentially after retrying the API call).</li>
                <li>If a chunk fails all API and compilation attempts, its generated `.tex` and logs are saved in `failed_chunks/`, a placeholder is inserted into the final document structure, and processing continues to the next chunk.</li>
                <li>If the *final* concatenated document fails to compile, an error is logged, and the final PDF won't be generated. The intermediate `.tex` file will still be available.</li>
                <li>Check `processing.log` and `failed_chunks/` for details on errors. You might need to manually fix issues in the source `.tex` files or adjust script parameters.</li>
            </ul>
        </li>
    </ul>

    <h4>7. Key Features:</h4>
    <ul>
        <li><strong>Chunk-Based Processing:</strong> Divides the PDF into multi-page chunks, suitable for models that handle PDF input directly.</li>
        <li><strong>AI-Driven Chunking:</strong> Attempts to use the selected AI to determine logical chunk boundaries based on document structure (falls back to fixed size).</li>
        <li><strong>Preamble Synthesis:</strong> Extracts preambles from successful chunks and uses AI to synthesize a unified final preamble (falls back to basic concatenation).</li>
        <li><strong>Single Model Per Run:</strong> Uses one selected AI model (Gemini, OpenAI, or Claude) for all AI tasks (chunking, translation, synthesis) in a given run.</li>
        <li><strong>Robust Retries:</strong> Includes retries for both API calls and LaTeX compilation steps for each chunk.</li>
        <li><strong>Graceful Chunk Failure:</strong> Inserts placeholders for completely failed chunks rather than stopping the entire process. Saves failed data for diagnosis.</li>
        <li><strong>State Management:</strong> Automatically saves and resumes progress using `state.json`.</li>
        <li><strong>Detailed Logging:</strong> Separates general processing logs (`processing.log`) from AI interaction logs (`prompts_and_responses.log`).</li>
        <li><strong>Command-Line Interface:</strong> Flexible control over provider, model, chunking, and directories.</li>
    </ul>
</div>

<!-- Removed comparison table -->

</body>
</html>
