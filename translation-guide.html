<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How to Translate a Math PDF (Co-written with Gemini 2.5 Pro)</title>
    <style>
        /* Basic styling for readability */
        body { font-family: sans-serif; line-height: 1.6; margin: 2em; max-width: 900px; margin-left: auto; margin-right: auto; } /* Added max-width and centering */
        h1, h2, h3, h4 { margin-top: 1.5em; color: #333; } /* Added h4 and color */
        h1 { border-bottom: 2px solid #eee; padding-bottom: 0.3em;} /* Added heading style */
        pre { background-color: #f8f8f8; padding: 1em; border: 1px solid #ddd; overflow-x: auto; white-space: pre-wrap; /* Wrap long lines */ word-wrap: break-word; /* Break words if needed */ font-size: 0.9em; border-radius: 4px; } /* Nicer pre style */
        code { font-family: monospace; background-color: #f0f0f0; padding: 0.2em 0.4em; border-radius: 3px;} /* Nicer code style */
        ul, ol { margin-left: 1.5em; }
        li { margin-bottom: 0.5em; }
        a { color: #007bff; text-decoration: none; } /* Link styling */
        a:hover { text-decoration: underline; }
        /* Style to visually separate Gemini's explanation */
        .gemini-explanation { border-left: 4px solid #4285F4; /* Google blue */ padding-left: 1em; margin-top: 1.5em; margin-bottom: 1.5em; background-color: #f9f9f9; border-radius: 4px; padding: 1em; } /* Enhanced styling */
        .requirements ul li strong { display: inline-block; min-width: 150px; } /* Align requirement titles */
        .warning { color: #dc3545; font-weight: bold; }
        .note { font-style: italic; color: #555; }
    </style>
</head>
<body>

<h1>How to Translate a Math PDF (Co-written with Gemini 2.5 Pro)</h1>

<p>Nowadays, multimodal AI models can take as input a math paper in a non-English language and output a translation directly in LaTeX. Below is a guide on this. Before proceeding, beware that there are always risks of hallucination or error from the AI and from the fact that this guide was written in April 2025 and was suboptimal even when written.</p>

<p>A quick note: for this to work, the core capability for this task has to exist in the multimodal AI itself. The guide below merely provides scaffolding to overcome the following two common challenges:</p>
<ul>
    <li><strong>Output Length Limits:</strong> AIs often can't generate the entire LaTeX for a long paper in one output, especially via API calls.</li>
    <li><strong>AI Confusion from Input Size:</strong> Processing hundreds of pages of math can sometimes lead to a degradation in translation ability or increased errors.</li>
</ul>

<p>We divide this guide into two main situations.</p>

<h2>Situation 1 (Easiest): Translating Short Papers (e.g., < 20 pages)</h2>

<p>If the paper is short, you can often translate it directly using a chat interface for any capable multimodal AI model (like Google's AI Studio for Gemini).</p>
<ol>
    <li><strong>Method:</strong> Upload the PDF directly into the chat interface.</li>
    <li><strong>Prompt:</strong> Use a prompt like this:
        <pre><code>Please translate the uploaded document into English LaTeX code. Ensure all mathematical formulas and environments are preserved correctly. Generate the complete LaTeX document, including a suitable preamble.</code></pre>
    </li>
    <li><strong>Process:</strong> Copy the AI's generated LaTeX output, paste it into your LaTeX editor (e.g., Overleaf), compile, and carefully review the resulting PDF.</li>
</ol>

<h2>Situation 2: Translation for Longer Papers via an API</h2>

<p>For longer papers, we must divide the paper into manageable chunks for translation. Using the Gemini API, it is possible to write code to automate this process. The philosophy here is that the scaffolding imposed on the AI model should be minimal, so as not to interfere with the AI's own ability to process and translate effectively.</p>

<p>The basic script workflow involves these steps:</p>
<ol>
    <li>Determine how to split the PDF into chunks (e.g., based on logical sections suggested by the AI, or simply using fixed page counts).</li>
    <li>Call the Gemini API for each chunk, sending the relevant pages (potentially as images or extracted text, depending on the chosen API capabilities and complexity) for translation into LaTeX format.</li>
    <li>Combine the resulting LaTeX pieces generated for each chunk into a single, final <code>.tex</code> file.</li>
</ol>

<!-- User's corrected paragraph -->
<p>We provide an example script written with Gemini:<a href="translator.py" download>translator.py</a>.
<!-- Make sure translator.py is in the same directory as this HTML file, or adjust the href path -->
In order the use this script, you first need to obtain a Gemini API key and set up a Python environment with the necessary libraries installed. </p>

<p>I'll let Gemini explain in more detail:</p>

<div class="gemini-explanation"> <!-- Start of Gemini Explanation -->
    <h3>How to Use the Multi-Model Python Script (translator.py)</h3>
    <p>(This script uses a tournament of models including Claude, Gemini, and GPT)</p>

    <h4>1. Requirements:</h4>
    <ul class="requirements">
        <li><strong>Python:</strong> Version 3.x recommended. Use a virtual environment (`venv`).</li>
        <li><strong>Python Packages:</strong> Install via pip:
            <pre><code>pip install google-generativeai anthropic openai pdf2image Pillow pypdf</code></pre>
        </li>
        <li><strong>Poppler:</strong> Required by `pdf2image`. Install system-wide (e.g., `brew install poppler`, `sudo apt-get install poppler-utils`). Ensure it's in your PATH. <span class="warning">Installation issues are common! Check logs if image conversion fails.</span></li>
        <li><strong>LaTeX Distribution:</strong> A working installation (TeX Live, MiKTeX) is **required**. The `pdflatex` command must be runnable from your terminal.</li>
        <li><strong>API Keys:</strong> You need API keys for the providers of the models you intend to use. Set them as environment variables:
            <ul>
                <li><code>ANTHROPIC_API_KEY</code> (for Claude models)</li>
                <li><code>OPENAI_API_KEY</code> (for GPT models)</li>
                <li><code>GOOGLE_API_KEY</code> (for Gemini models)</li>
            </ul>
             Get keys from their respective websites (Anthropic Console, OpenAI Platform, Google AI Studio). The script will skip models from providers whose keys are not found/set.
        </li>
    </ul>

    <h4>2. Configuration (Optional):</h4>
    <ul>
        <li><strong>Model Configs:</strong> Edit the `get_model_configs()` function within `translator.py` to add or update model details. Use `--list-models` to see current options.</li>
        <li><strong>Default Model Queue:</strong> Change the `DEFAULT_MODEL_QUEUE` list in the script to modify the default order in which models are tried.</li>
        <li><strong>Working Directory:</strong> Outputs go to a hardcoded directory named `latex_processing` created in the same directory where the script is run. <span class="note">(This version does not have a `--working-dir` flag).</span></li>
        <li><strong>Retries/Cycles:</strong> Adjust `MAX_RETRIES_PER_MODEL` and `MAX_MODEL_CYCLES` constants in the script for more or less aggressive attempts per page.</li>
    </ul>


    <h4>3. Running the Script:</h4>
    <p>Open your terminal/command prompt in the directory containing `translator.py`.</p>
    <p><strong>Basic Command (using default model queue):</strong></p>
    <pre><code>python translator.py [path/to/your/input.pdf]</code></pre>
    <p><strong>Command with a Preferred Starting Model:</strong></p>
    <pre><code>python translator.py [path/to/your/input.pdf] --model MODEL_KEY</code></pre>
    <ul>
        <li>The <strong>Input PDF Path</strong> is the first (optional) argument. If omitted, it defaults to `input.pdf` in the script's directory.</li>
        <li>The <code>--model</code> flag specifies which AI model you want to try *first*. The script will still fall back to other models in the `DEFAULT_MODEL_QUEUE` if the preferred one fails.</li>
    </ul>

    <p><strong>Example Commands:</strong></p>
    <pre><code class="language-bash"># Translate my_paper.pdf using the default model queue
python translator.py my_paper.pdf

# Translate input.pdf, trying gemini-2-5-pro-exp *first*, then others in the default queue
python translator.py --model gemini-2-5-pro-exp

# Translate paper_v2.pdf, trying gpt-4o *first*, then others
python translator.py paper_v2.pdf --model gpt-4o</code></pre>

    <p class="note"><em>Use quotes around paths with spaces. You might need <code>python3</code> instead of <code>python</code>.</em></p>

    <h4>4. Command-Line Options (Flags):</h4>
    <ul>
        <li><code>input_pdf</code> (Positional, Optional): Path to the input PDF (defaults to `input.pdf`).</li>
        <li><code>--model MODEL_KEY</code> or <code>-m MODEL_KEY</code>: Specify a preferred model key (e.g., `claude-3-7`, `gpt-4o`, `gemini-2-5-pro-exp`) to try *first* for each page. The script will cycle through the rest of the default queue if this model fails. Use <code>--list-models</code> to see available keys.</li>
        <li><code>--list-models</code> or <code>-l</code>: Display available model keys configured in the script, then exit.</li>
    </ul>

    <h4>5. Available Models (Examples from Script):</h4>
    <p>The script supports multiple AI models configured in `get_model_configs()`. Examples include:</p>
    <ul>
        <li><strong>Claude Models:</strong> claude-3-7 (Sonnet), claude-3-sonnet, claude-3-haiku</li>
        <li><strong>Gemini Models:</strong> gemini-2-5-pro-exp, gemini-2-5-pro-preview, gemini-1-5-pro, gemini-1-5-flash</li>
        <li><strong>OpenAI Models:</strong> gpt-4o, gpt-4o-mini</li>
    </ul>
    <p>Use <code>--list-models</code> to see the complete, up-to-date list of model keys available in your version of the script and the default processing order.</p>

    <h4>6. Output Files (in `./latex_processing` Directory):</h4>
    <ul>
        <li><code>cumulative_good.tex</code>: The latest successfully compiled LaTeX source (including placeholders for failed pages).</li>
        <li><code>cumulative_good.pdf</code>: The PDF generated from `cumulative_good.tex`. Represents the last known good state.</li>
        <li><code>processing.log</code>: Detailed log of the translation process, including model attempts and errors.</li>
        <li><code>state.json</code>: Stores the last completed page number for resuming.</li>
        <li><code>model_stats.json</code>: Tracks which models successfully translated which pages, and records page failures.</li>
        <li><code>latex_aux/</code>: Directory containing auxiliary files from LaTeX compilation (`.log`, `.aux`, `.pdf`, etc.).</li>
        <li><code>failed_pages/</code>: Contains images (`.jpg`) of pages that failed all translation attempts across all models or failed compilation.</li>
        <li><code>*.log</code> (e.g., `compile_fail_page_*.log`, `report_compile.log`, `failed_appendix_compile.log`): Specific logs for compilation failures of main doc or reports.</li>
        <li><code>model_report.tex</code> / <code>failed_pages_appendix.tex</code>: Source files for generated reports.</li>
        <li><code>*_final.pdf</code> / <code>*_final_with_report.pdf</code>: The final output PDF, potentially merged with the performance report and failed page appendix. (Located in the `latex_processing` directory).</li>
        <li><code>current_attempt.tex</code>: Temporary file holding the LaTeX for the page currently being processed and compiled.</li>
    </ul>

    <h4>7. Resuming and Error Handling:</h4>
    <ul>
        <li><strong>Resuming:</strong> Simply run the script again with the same input PDF and preferred model flag (if used). It will read `state.json` and continue from the last successfully completed page.</li>
        <li><strong>Failures:</strong>
            <ul>
                <li>If a model's API call fails or returns unusable content, the script tries the next model in the queue, retrying each model several times and cycling through the queue multiple times.</li>
                <li>If a model returns LaTeX successfully, but that LaTeX causes a compilation error when added, the script treats that page as failed, inserts a placeholder, saves the original page image, and moves to the next page.</li>
                <li>If *all models fail* all retries/cycles for a page, its image is saved in `failed_pages/`, a placeholder is inserted in the LaTeX, and processing continues to the next page.</li>
                <li>Review `processing.log`, `model_stats.json`, and any `compile_fail_*.log` files to understand failures.</li>
                <li>Manually edit `cumulative_good.tex` *after* the script completes to fix translation errors or replace placeholders if desired, then recompile manually.</li>
            </ul>
        </li>
    </ul>


    <h3>Key Features of the Script</h3>
    <ul>
        <li><strong>Multi-Model Tournament:</strong> Leverages a queue of models (Claude, GPT, Gemini) for increased robustness and success rate.</li>
        <li><strong>Robust Retry/Cycle Mechanism:</strong> Retries individual models and cycles through the entire model queue multiple times before giving up on a page.</li>
        <li><strong>Intelligent Fallback:</strong> Inserts placeholders and saves original images for pages that fail all AI attempts or cause compilation errors, ensuring the process continues.</li>
        <li><strong>Configurable Model Preference:</strong> Allows specifying a preferred model to try first via command line.</li>
        <li><strong>Page-by-Page Processing:</strong> Handles long documents by processing one page at a time.</li>
        <li><strong>Image-Based Translation:</strong> Converts PDF pages to images for multimodal AI input.</li>
        <li><strong>Contextual Prompting:</strong> Provides models with context from the previous page's LaTeX output.</li>
        <li><strong>Inline Compilation Checks:</strong> Attempts `pdflatex` compilation after each page is added to catch LaTeX errors early.</li>
        <li><strong>Graceful Failure Handling:</strong> Designed to complete the document even if some pages fail, using placeholders.</li>
        <li><strong>State Management:</strong> Automatically saves and resumes progress using `state.json`.</li>
        <li><strong>Performance Reporting:</strong> Generates statistics (`model_stats.json`) and a final PDF report summarizing model performance and page failures.</li>
        <li><strong>Report Merging:</strong> Automatically merges the performance report and an appendix of failed page images into the final PDF.</li>
        <li><strong>Command-Line Interface:</strong> Flexible control via CLI arguments.</li>
        <li><strong>Detailed Logging:</strong> Records progress, model attempts, and errors to `processing.log`.</li>
    </ul>
</div> <!-- End of Gemini Explanation -->

</body>
</html>
